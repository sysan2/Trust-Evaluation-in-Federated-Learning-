{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588b7059",
   "metadata": {},
   "source": [
    "This code is for the client without any attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d6a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from create_MNIST_datasets import get_MNIST, plot_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b313c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc270d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Starting Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "â™«â™«â™« > \u001b[95mSTEP 1:\u001b[0m Send the following code to your Duet Partner!\n",
      "\n",
      "import syft as sy\n",
      "duet = sy.join_duet(loopback=True)\n",
      "\n",
      "â™«â™«â™« > Connecting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syeda/courses/fl/lib/python3.8/site-packages/aiortc/rtcdtlstransport.py:211: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  _openssl_assert(lib.SSL_CTX_use_certificate(ctx, self._cert._x509) == 1)  # type: ignore\n",
      "/Users/syeda/courses/fl/lib/python3.8/site-packages/aiortc/rtcdtlstransport.py:186: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  value=certificate_digest(self._cert._x509),  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n",
      "\n",
      "tensor(2.2993, grad_fn=<AddBackward0>) Requests: 0   Messages: 3  Request Handlers: 0                                \n",
      "tensor(2.2964, grad_fn=<AddBackward0>) Requests: 0   Messages: 3  Request Handlers: 0                                \n",
      "â™«â™«â™« > DUET LIVE STATUS  -  Objects: 0  Requests: 0   Messages: 13  Request Handlers: 1                                \r"
     ]
    }
   ],
   "source": [
    "duet = sy.launch_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db13a426",
   "metadata": {},
   "source": [
    "Each client will launch the duet and from the server side the server would join the duet. When a connection is established, then it would show connected in the drop box and a live status which is shown by a star, would animate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494f1b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_iid_train_dls = torch.load('client0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c56f1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.fc = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model_client = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a693565",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = duet.store[0].get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfead2e",
   "metadata": {},
   "source": [
    "By duet.get - the client got the initial model parameters and the would now run the code upto the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a1794d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.utils.vector_to_parameters(x,model_client.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2095ec1",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9393741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_classifier(predictions,labels):\n",
    "    \n",
    "    m = nn.LogSoftmax(dim=1)\n",
    "    loss = nn.NLLLoss(reduction=\"mean\")\n",
    "    \n",
    "    return loss(m(predictions) ,labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2897a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3034, grad_fn=<AddBackward0>)\n",
      "tensor(2.3021, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/nn8rqkp92c3413g9w6xsc9y00000gn/T/ipykernel_47265/594803556.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3007, grad_fn=<AddBackward0>)\n",
      "tensor(2.2979, grad_fn=<AddBackward0>)\n",
      "tensor(2.2948, grad_fn=<AddBackward0>)\n",
      "tensor(2.2932, grad_fn=<AddBackward0>)\n",
      "tensor(2.2915, grad_fn=<AddBackward0>)\n",
      "tensor(2.2898, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_data = mnist_iid_train_dls\n",
    "\n",
    "epochs = 10\n",
    "lr =0.1\n",
    "optimizer=optim.SGD(model_client.parameters(),lr=lr) \n",
    "for e in range(epochs): \n",
    "    total_loss = 0\n",
    "    for idx, (features,labels) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        predictions= model_client(features)\n",
    "        loss=loss_classifier(predictions,labels)\n",
    "        total_loss+=loss;\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(total_loss)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28340a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0264,  0.0143, -0.0260,  ..., -0.0160,  0.0132,  0.0203],\n",
       "         [ 0.0353,  0.0115,  0.0053,  ..., -0.0323, -0.0274,  0.0146],\n",
       "         [ 0.0109, -0.0297, -0.0219,  ...,  0.0022,  0.0330,  0.0248],\n",
       "         ...,\n",
       "         [-0.0069, -0.0276, -0.0028,  ..., -0.0208, -0.0108, -0.0067],\n",
       "         [-0.0155, -0.0122, -0.0277,  ..., -0.0130, -0.0083,  0.0113],\n",
       "         [ 0.0288,  0.0157, -0.0312,  ..., -0.0159, -0.0133,  0.0251]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0264,  0.0027, -0.0145, -0.0112,  0.0007, -0.0292, -0.0100,  0.0302,\n",
       "         -0.0216,  0.0206], requires_grad=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_client.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91d14ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = torch.nn.utils.parameters_to_vector(model_client.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d445b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syeda/courses/fl/lib/python3.8/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<syft.proxy.torch.TensorPointer at 0x13b8cc4c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.send(duet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c7970c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Description</th>\n",
       "      <th>object_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;UID: 03088f0e51b844a984e93c13d91c333b&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ID Tags Description  \\\n",
       "0  <UID: 03088f0e51b844a984e93c13d91c333b>   []               \n",
       "\n",
       "              object_type  \n",
       "0  <class 'torch.Tensor'>  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6c7c4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-03T13:24:38.333704-0500][CRITICAL][logger]][47265] You do not have permission to .get() Object with ID: <UID: 03088f0e51b844a984e93c13d91c333b>Please submit a request.\n",
      "[2022-12-03T13:24:38.341099-0500][CRITICAL][logger]][47265] You do not have permission to .get() Object with ID: <UID: 03088f0e51b844a984e93c13d91c333b>Please submit a request.\n"
     ]
    }
   ],
   "source": [
    "duet.requests.add_handler(action=\"accept\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a18ef",
   "metadata": {},
   "source": [
    "The client has sent the model update by .send(duet) and has accepted the request from the server to access the updated model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ce151",
   "metadata": {},
   "source": [
    "The client can see any updates or any data sent from the server in the live status. Also in the live status the request updates from the server will also be shown. If there is no updates then Objects: 0  Requests: 0   Messages: 0  Request Handlers: 0. And if there is any update on any of them the number  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30291a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
